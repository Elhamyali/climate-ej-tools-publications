---
title: How climate and envrionmental justice tools are used in research studies
author:
  - name: Elham Ali
    corresponding: true
    roles:
      - Researcher
      - Data Storytelling
      - Human-centered Design
      - Data Visualization
    affiliations:
      - Public Environmental Data Project
license: CC BY-SA 4.0
keywords:
  - climate change
  - environmental justice
  - digital tools
  - research studies
  - academic research
date: last-modified
abstract: |
  TBD
keypoints:
  - climate change
  - environmental justice
  - digital tools
  - research studies
  - academic research
citation:
  container-title: Public Environmental Data Project
draft: false
bibliography: references.bib
code-fold: true
# reference-location: margin
# citation-location: margin
echo: true
warning: false
---

## Background

When public climate & EJ evidence disappears (removed, restricted, or altered), a decade of downstream knowledge becomes harder to verify, reproduce, teach, or apply—especially for communities and decisions that most need it.\
This analysis looks at how many studies have used these tools, their topics, and their use cases.

## Questions

Here are the key questions explored in this analysis:

-   How many research papers or studies have cited or relied on five U.S. federal climate and environmental tools that are threatened, not being maintained, or no longer available?\
-   What research topics are associated with each tool, and how do these topics vary by citation frequency?\
-   In what ways are these tools applied in research?

## Data Sources

The climate tools assessed are:

-   CEQ’s **Climate and Economic Justice Screening Tool (CEJST)**\
-   EPA’s **EJScreen**\
-   USDA Forest Service’s **Climate Risk Viewer (CRV)**\
-   FEMA’s **Future Risk Index (FRI)**\
-   CDC’s **Environmental Justice Index (EJI)**

The data for this project comes from Google scholar and last refreshed on August 29, 2025.

Original raw datasets are saved in the `data/` folder. This script reduces and cleans those datasets to prepare them for analysis.

------------------------------------------------------------------------

## Cleaning

I start by loading the packages needed for file handling, data wrangling, and visualization.

```{r load-libraries, echo = TRUE, message = FALSE, results = 'markup'}
#| label: load-libraries

## Folder structure helpers
library(here)
library(ezknitr)

## Data import & cleaning
library(tidyverse)  # includes dplyr, purrr, readr, tidyr, stringr, etc.
library(janitor)
library(lubridate)
library(janitor)
# library(rlang)

## Visualization
library(highcharter)
library(igraph)
library(RColorBrewer)
library(htmlwidgets)
library(gt)
```

### Import raw data

I import all .csv files from the `data/` folder, then save them as .rds files into `output/`. This preserves their structure and speeds up future reads.

```{r import-data, echo = TRUE, message = FALSE, results = 'markup'}
#| label: import-data

# List all CSV files
csv_files <- list.files(here("data"), pattern = "\\.csv$", full.names = TRUE)

# Read into a list of dataframes
datasets <- map(csv_files, read.csv)
names(datasets) <- tools::file_path_sans_ext(basename(csv_files))

# Save each dataset as .rds in output/
walk2(
  datasets,
  names(datasets),
  ~ saveRDS(.x, here("output", paste0(.y, ".rds")))
)
```

### Merge EJScreen datasets

Google Scholar caps search results at 1,000 records [@harzing2025].

Because EJScreen returned more than this, I split the results into four files (ejscreen_1–ejscreen_4). Here I combine them into one dataframe.

```{r ejscreen-cleaning, echo = TRUE, message = FALSE, results = 'markup'}
#| label: ejscreen-cleaning

# List EJScreen RDS files
ejscreen_files <- here("output") |>
  list.files(pattern = "^ejscreen_\\d+\\.rds$", full.names = TRUE)

# Read, tag, clean variable names
ejscreen_list <- map(ejscreen_files, ~ {
  readRDS(.x) |>
    mutate(source_file = tools::file_path_sans_ext(basename(.x))) |>
    janitor::clean_names()
})

# Merge into one dataframe
ejscreen_all <- bind_rows(ejscreen_list)

# Quick check on dimensions
dim(ejscreen_all)  

```

### Clean and tag all other tools

I apply the same cleaning process to the other tools (CEJST, CRV, EJI, FRI):

-   Standardize variable names to snake_case
-   Add a source_file column for provenance
-   Guess variable types (integers, doubles, dates, etc.)

```{r remaining-tools-cleaning, echo = TRUE, message = FALSE, results = 'markup'}
#| label: remaining-tools-cleaning

# Helper function: read, clean names, add provenance, type-convert
read_clean_tag <- function(path) {
  df <- readRDS(path) |>
    janitor::clean_names()

  source_stem <- tools::file_path_sans_ext(basename(path))  # e.g., "ejscreen_1" or "cejst"

  df |>
    mutate(
      source_file = source_stem,
      tool_name   = sub("_\\d+$", "", source_stem)  # "ejscreen_1" -> "ejscreen"; "cejst" -> "cejst"
    ) |>
    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>
    mutate(across(where(is.character), trimws))
}
# 
# # Rebuild ejscreen_all if needed
# if (!exists("ejscreen_all")) {
#   ejs_files <- list.files(here("output"), pattern = "^ejscreen_\\d+\\.rds$", full.names = TRUE)
#   ejscreen_all <- ejs_files |> map(read_clean_tag) |> list_rbind()
# }

# Process other tools
other_tools <- c("cejst", "crv", "eji", "fri")
other_paths <- here("output", paste0(other_tools, ".rds"))

other_list <- other_paths |>
  set_names(~ tools::file_path_sans_ext(basename(.x))) |>
  map(read_clean_tag)

```

### Final merge and labeling

I now merge all tools into a single dataset. Additional steps:

-   Ensure consistent date formats
-   Map tool codes (ejscreen, cejst, etc.) to their full names
-   Drop empty placeholder columns

```{r final-cleaning, echo = TRUE, message = FALSE, results = 'markup'}
#| label: final-cleaning

# Helper: coerce anything date-like into Date
to_Date <- function(x) {
  if (inherits(x, "Date"))    return(x)
  if (inherits(x, "POSIXt"))  return(as_date(x))
  if (is.numeric(x))          return(as_date(as.POSIXct(x, origin = "1970-01-01", tz = "UTC")))
  if (is.character(x)) {
    suppressWarnings({
      dt <- ymd_hms(x, quiet = TRUE, tz = "UTC")
      idx <- is.na(dt); if (any(idx)) dt[idx] <- ymd(x[idx], quiet = TRUE)
      idx <- is.na(dt); if (any(idx)) dt[idx] <- mdy(x[idx], quiet = TRUE)
      idx <- is.na(dt); if (any(idx)) dt[idx] <- dmy(x[idx], quiet = TRUE)
    })
    return(as_date(dt))
  }
  as_date(as.character(x))
}

# Apply date coercion
if ("query_date" %in% names(ejscreen_all)) {
  ejscreen_all <- ejscreen_all %>% mutate(query_date = to_Date(query_date))
}

if (exists("ejscreen_all") && "query_date" %in% names(ejscreen_all)) {
  ejscreen_all <- ejscreen_all %>% mutate(query_date = to_Date(query_date))
}

if (exists("other_list")) {
  other_list <- other_list %>%
    map(~ if ("query_date" %in% names(.x)) mutate(.x, query_date = to_Date(query_date)) else .x)
} else {
  other_list <- list()
}

# Merge all datasets
all_tools <- bind_rows(ejscreen_all, !!!other_list)

# Map tool codes to full names
all_tools <- all_tools %>%
  mutate(
    source_file = if_else(is.na(source_file) | source_file == "",
                          coalesce(source_file, tool_name), source_file),
    base_tool   = sub("_\\d+$", "", source_file),   # ejscreen_1 -> ejscreen
    tool_name   = dplyr::recode(
      base_tool,
      ejscreen = "EJScreen",
      cejst    = "Climate and Economic Justice Screening Tool (CEJST)",
      crv      = "Climate Risk Viewer",
      fri      = "Future Risk Index (FRI)",
      eji      = "Environmental Justice Index (EJI)",
      .default = tool_name  # keep whatever was there if not matched
    )
  ) %>%
  select(-base_tool)

# Drop empty placeholder columns
all_tools <- all_tools %>%
  select(-any_of(c("issn", "citation_url", "volume", "issue", "start_page", "end_page")))

# Save combined dataset
glimpse(all_tools)
saveRDS(all_tools, here("output", "all_tools.rds"))
write_csv(all_tools, here("output", "all_tools.csv"))

```

## Analysis

I will look at each question one by one and clean the data as I go. I will organize the data during the analysis before exploring the results. I'll also export intermediate results into tidy CSV files so they are ready for further visualization and exploration.

### Research question 1

How many research papers or studies have cited or relied on five U.S. federal climate and environmental tools that are threatened, not being maintained, or no longer available?

#### Total number of studies

```{r descriptive-stat-1, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-1

nrow (all_tools)

write_csv(tibble(total_studies = nrow(all_tools)), here("viz", "total_studies.csv"))
```

#### Count of studies per tool

```{r descriptive-stat-2, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-2

df_tool <- all_tools %>%
  count(tool_name, name = "n_studies") %>%
  arrange(desc(n_studies))

hchart(df_tool, "column", hcaes(x = tool_name, y = n_studies)) %>%
  hc_title(text = "Number of studies by tool") %>%
  hc_subtitle(text = "Total count of research papers citing or relying on each tool") %>%
  hc_xAxis(title = list(text = "Tool")) %>%
  hc_yAxis(title = list(text = "Number of studies"), allowDecimals = FALSE) %>%
  hc_tooltip(pointFormat = "<b>{point.y}</b> studies") %>%
  hc_legend(enabled = FALSE) %>%
  hc_exporting(enabled = TRUE)

write_csv(df_tool, here("viz", "count_per_tool.csv"))
```

#### Count of studies per year per tool

```{r descriptive-stat-3, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-3

df_year_tool <- all_tools %>%
  filter(!is.na(year)) %>%
  count(tool_name, year, name = "n_studies") %>%
  arrange(tool_name, year)

hchart(
  df_year_tool,
  "line",
  hcaes(x = year, y = n_studies, group = tool_name)
) %>%
  hc_title(text = "Studies over time by tool") %>%
  hc_subtitle(text = "Annual counts of studies citing or relying on each tool") %>%
  hc_xAxis(title = list(text = "Year"), allowDecimals = FALSE) %>%
  hc_yAxis(title = list(text = "Number of studies"), allowDecimals = FALSE) %>%
  hc_tooltip(shared = TRUE, crosshairs = TRUE,
             headerFormat = "<b>Year: {point.key}</b><br/>",
             pointFormat = "{series.name}: <b>{point.y}</b><br/>") %>%
  hc_legend(title = list(text = "Tool")) %>%
  hc_exporting(enabled = TRUE)

write_csv(df_year_tool, here("viz", "count_per_year_per_tool.csv"))
```

#### Top 10 most cited studies overall per tool

```{r descriptive-stat-4, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-4

df_top_cites <- all_tools %>%
  filter(!is.na(cites)) %>%
  group_by(tool_name) %>%
  slice_max(order_by = cites, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  select(tool_name, year, title, cites, authors) %>%
  arrange(tool_name, desc(cites))

# Show table (optional)
df_top_cites

# Ensure viz/ exists, then export
if (!dir.exists(here::here("viz"))) dir.create(here::here("viz"), recursive = TRUE)
readr::write_csv(df_top_cites, here::here("viz", "top10_cites_per_tool.csv"))
```

#### Top 10 highest `cites_per_year` per tool

This shows which studies are “most cited relative to their age.”

```{r descriptive-stat-5, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-5

df_top_cpy <- all_tools %>%
  filter(!is.na(cites_per_year)) %>%
  group_by(tool_name) %>%
  slice_max(order_by = cites_per_year, n = 10, with_ties = FALSE) %>%
  ungroup()

df_top_cpy %>% 
  select(tool_name, year, title, cites_per_year, cites, authors) %>%
  arrange(tool_name, desc(cites_per_year))

hchart(
  df_top_cpy,
  "bar",
  hcaes(x = reorder(title, cites_per_year), y = cites_per_year, group = tool_name)
) %>%
  hc_title(text = "Top 10 studies by cites per year, per tool") %>%
  hc_subtitle(text = "Highlights studies with the highest relative citation rates") %>%
  hc_xAxis(title = list(text = "Study title"), labels = list(enabled = FALSE)) %>%  # hide long labels
  hc_yAxis(title = list(text = "Cites per year")) %>%
  hc_tooltip(pointFormat = "<b>{point.y}</b> cites/year<br>{point.tool_name}<br>") %>%
  hc_exporting(enabled = TRUE)

write_csv(df_top_cpy, here("viz", "top10_cites_per_year_per_tool.csv"))
```

#### Average `cites_per_year` over time per tool

This shows whether more recent papers citing a tool are getting traction.

```{r descriptive-stat-6, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-6

df_avg_cpy <- all_tools %>%
  filter(!is.na(year), !is.na(cites_per_year)) %>%
  group_by(tool_name, year) %>%
  summarise(avg_cpy = mean(cites_per_year, na.rm = TRUE), .groups = "drop")

hchart(df_avg_cpy, "line", hcaes(x = year, y = avg_cpy, group = tool_name)) %>%
  hc_title(text = "Average cites per year over time, by tool") %>%
  hc_yAxis(title = list(text = "Average cites per year")) %>%
  hc_exporting(enabled = TRUE)

write_csv(df_avg_cpy, here("viz", "avg_cites_per_year_over_time.csv"))
```

#### Publishers

Which top 10 publishers/platforms appear most frequently?

```{r descriptive-stat-7, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-7

df_10_publishers <- all_tools %>%
  filter(publisher != "") %>%
  count(publisher, sort = TRUE) %>%
  slice_head(n = 10)

df_10_publishers

write_csv(df_10_publishers, here("viz", "top_publishers.csv"))
```

List all publishers from the sample.

```{r descriptive-stat-8, echo = TRUE, message = FALSE, results = 'markup'}
#| label: descriptive-stat-8

df_all_publishers <- all_tools %>%
  filter(publisher != "") %>%
  count(publisher, sort = TRUE)

write_csv(df_all_publishers, here("viz", "all_publishers.csv"))
```


#### Stratified random sampling & inter-rater agreement

To answer **Research Questions 2 and 3**, I will work with a collaborator to double-code a subset of research studies. Together, we will manually assign each study to a **topic category** and a **tool use type**. This step establishes **inter-rater agreement** (agreement between coders beyond chance).

For sampling, I am taking **10% of the full dataset (2,170 studies) = 217 studies**. Sampling will be stratified by tool `tool_name`. The entire 222-study subset will be double-coded.

A helpful guide is *Stratified Sampling in R: A Practical Guide with Base R and dplyr* [@sanderson2024].

I considered two allocation approaches:

1.  **Proportional-only allocation** (222 total).

    -   Each tool receives samples proportional to its share of the corpus.

    -   This preserves representativeness but leaves small tools (e.g., CRV, FRI) with too few cases to analyze.

2.  **Proportional with a minimum floor** (222 total, MIN = 10).

    -   Every tool gets at least 10 studies (or all available, if fewer).

    -   Larger tools (EJScreen, CEJST) give up a small number of cases to make room.

I chose the **minimum floor approach**, since it ensures smaller tools are represented enough for meaningful analysis and coder agreement checks.

**Tradeoff:**

-   *Proportional-only*: best if the goal is a population-level snapshot but leaves tiny strata nearly invisible.

-   *Floor allocation*: best if we need to compare across tools.

The final distribution for the double-coded sample is:

```{r}
set.seed(2025)

# Load the dataset
all_tools <- readRDS(here("output", "all_tools.rds")) %>%
  janitor::clean_names() %>%
  mutate(
    year = as.integer(year),
    study_id = dplyr::row_number()
  )

# Parameters
TOTAL <- nrow(all_tools)               # should be 2212
SAMPLE_TOTAL <- ceiling(TOTAL * 0.10)  # 10% => 217
MIN_PER_TOOL <- 10                     # floor per tool
EXPAND_TOTAL <- TRUE                   # expand total if needed

# --------------------------------------------------------------------
# Allocation with per-tool minimums, then proportional distribution
# --------------------------------------------------------------------
tool_counts <- all_tools %>%
  count(tool_name, name = "n_tool") %>%
  arrange(desc(n_tool))

# Floors (min 10, capped at availability)
floor_alloc <- tool_counts %>%
  mutate(floor = pmin(n_tool, MIN_PER_TOOL))

reserved <- sum(floor_alloc$floor)

if (reserved > SAMPLE_TOTAL) {
  if (EXPAND_TOTAL) {
    SAMPLE_TOTAL <- reserved
  } else {
    stop("Minimum per-tool exceeds total sample. Lower MIN_PER_TOOL or allow expand.")
  }
}

remaining <- SAMPLE_TOTAL - reserved

# Proportional extras after floors
alloc <- floor_alloc %>%
  mutate(capacity = pmax(0, n_tool - floor))

if (remaining > 0 && sum(alloc$capacity) > 0) {
  alloc <- alloc %>%
    mutate(
      weight     = capacity / sum(capacity),
      extra_raw  = remaining * weight,
      extra_base = floor(extra_raw),
      remainder  = extra_raw - extra_base
    )
  leftover <- remaining - sum(alloc$extra_base)
  alloc <- alloc %>%
    arrange(desc(remainder)) %>%
    mutate(extra = extra_base + if_else(row_number() <= leftover, 1L, 0L)) %>%
    arrange(desc(n_tool))
} else {
  alloc <- alloc %>% mutate(extra = 0L)
}

# Final allocation
alloc <- alloc %>%
  mutate(alloc = pmin(n_tool, floor + extra))

tool_alloc <- alloc %>%
  select(tool_name, n_tool, alloc)

stopifnot(sum(tool_alloc$alloc) == SAMPLE_TOTAL)

# --------------------------------------------------------------------
# Stratified random sample
# --------------------------------------------------------------------
orig_cols <- names(all_tools)

sample_primary <- all_tools %>%
  inner_join(tool_alloc, by = "tool_name") %>%
  group_by(tool_name) %>%
  group_modify(~ dplyr::slice_sample(.x, n = unique(.x$alloc))) %>%
  ungroup() %>%
  mutate(
    double_code   = TRUE,
    topic_cat     = NA_character_,
    tool_use_type = NA_character_
  ) %>%
  select(all_of(orig_cols), double_code, topic_cat, tool_use_type)

stopifnot(nrow(sample_primary) == SAMPLE_TOTAL)

# --------------------------------------------------------------------
# Save outputs (to output/ folder)
# --------------------------------------------------------------------
sample_primary %>% count(tool_name) %>% arrange(desc(n)) %>% print()

readr::write_csv(tool_alloc,     here("output", "alloc_with_min_floor.csv"))
readr::write_csv(sample_primary, here("output", "double_code_sample.csv"))

```

To support manual coding, I drafted an early codebook with candidate topic and tool use categories. Definitions, inclusion/exclusion criteria, and examples will be added as the coding scheme is refined.

```{r}
# Starter codebook for manual coding
topic_levels <- c(
  "Air quality / pollution",
  "Health impacts",
  "Extreme heat / climate risk",
  "Transportation",
  "Land use / housing",
  "Energy / utilities",
  "Water / flooding",
  "Equity / policy & governance"
)

tool_use_levels <- c(
  "Applied (used for analysis/targeting)",
  "Evaluated (validated/benchmarked the tool)",
  "Critiqued (limitations/methodological critique)",
  "Mentioned/Context (not used for analysis)"
)

codebook <- tibble::tibble(
  variable   = c(rep("topic_cat", length(topic_levels)),
                 rep("tool_use_type", length(tool_use_levels))),
  code       = c(topic_levels, tool_use_levels),
  definition = NA_character_,   # fill in
  include_if = NA_character_,   # fill in
  exclude_if = NA_character_,   # fill in
  examples   = NA_character_    # fill in small examples/keywords
)

readr::write_csv(codebook, here("output", "codebook_topic_tool_use.csv"))
```

### Research question 2

What research topics are associated with each tool, and how do these topics vary by citation frequency?

Coming soon!

### Research question 3

In what ways are these tools applied in research?

Coming soon!
